#FL with FedAVG and CKKS (FedARCH) 20 rounds and 10 clients 99% accuracy
import tenseal as ts
from datetime import datetime 
import random
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torchvision.models as models
import numpy as np
from torch.utils.data import random_split, DataLoader


def plot_metrics(train_losses_all, train_accuracies_all, num_rounds):
    # Flatten the nested lists
    train_losses_all_flat = [np.mean(client_losses, axis=0) for client_losses in train_losses_all]
    train_accuracies_all_flat = [np.mean(client_accuracies, axis=0) for client_accuracies in train_accuracies_all]

    rounds = list(range(1, num_rounds + 1))
    
    plt.figure(figsize=(12, 5))

    # Plot training accuracies
    plt.subplot(1, 2, 1)
    for client_idx, client_accuracies in enumerate(train_accuracies_all_flat):
        plt.plot(rounds, client_accuracies, label=f'Client {client_idx + 1}')
    plt.xlabel('Rounds')
    plt.ylabel('Accuracy')
    plt.title('Training Accuracies')
    plt.legend()

    # Plot training losses
    plt.subplot(1, 2, 2)
    for client_idx, client_losses in enumerate(train_losses_all_flat):
        plt.plot(rounds, client_losses, label=f'Client {client_idx + 1}')
    plt.xlabel('Rounds')
    plt.ylabel('Loss')
    plt.title('Training Losses')
    plt.legend()

    plt.tight_layout()
    plt.show()

now = datetime.now()
start_time = now.strftime("%H:%M:%S")

# Define BrainTumorResNet model
class BrainTumorResNet(nn.Module):
    def __init__(self):
        super(BrainTumorResNet, self).__init__()
        self.base_model = models.resnet18(pretrained=True)
        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, 4)  # Adjust for 4 classes

    def forward(self, x):
        x = self.base_model(x)
        return x  # Output raw logits (we'll apply softmax in the loss function)

# Evaluate model
def evaluate_model(model, dataloader):
    model.eval()
    correct = 0
    total = 0
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for inputs, labels in dataloader:
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            all_preds.extend(predicted.numpy())
            all_labels.extend(labels.numpy())
    accuracy = correct / total
    return accuracy, all_preds, all_labels

# Calculate confusion matrix and classification report
def calculate_metrics(all_preds, all_labels):
    cm = confusion_matrix(all_labels, all_preds)
    cr = classification_report(all_labels, all_preds, target_names=['Meningioma', 'Glioma', 'Pituitary', 'NoTumor'])
    return cm, cr

# Encrypt weights
def encrypt_weights(weights, context):
    encrypted_weights = {}
    for k, v in weights.items():
        vector = ts.ckks_vector(context, v.numpy().flatten().tolist())
        encrypted_weights[k] = vector
    return encrypted_weights

# Decrypt weights
def decrypt_weights(encrypted_weights, context, weight_shapes):
    decrypted_weights = {}
    for k, v in encrypted_weights.items():
        decrypted_vector = v.decrypt()
        decrypted_weights[k] = torch.tensor(decrypted_vector).view(weight_shapes[k])
    return decrypted_weights
def aggregate_weights_simple(weights_list):
    temp = 1/len(weights_list)
    avg_weights = {}
    for k in weights_list[0].keys():
        avg_weights[k] = sum([weights[k] for weights in weights_list]) * temp
    return avg_weights

# Federated learning with simple FedAvg
def federated_learning_simple_fedavg(num_clients=10, num_rounds=20, epochs_per_round=1):
    # Setup CKKS context
    context = setup_ckks_context()

    # Data augmentation and preprocessing
    data_transforms = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    train_path = "kaggle/Training"
    test_path = "kaggle/Testing"

    train_set = datasets.ImageFolder(root=train_path, transform=data_transforms)
    test_set = datasets.ImageFolder(root=test_path, transform=data_transforms)

    # Split data among clients
    client_data_sizes = [len(train_set) // num_clients for _ in range(num_clients)]
    for i in range(len(train_set) % num_clients):
        client_data_sizes[i] += 1

    client_datasets = random_split(train_set, client_data_sizes)
    client_loaders = [DataLoader(client_data, batch_size=32, shuffle=True) for client_data in client_datasets]
    test_loader = DataLoader(test_set, batch_size=32, shuffle=False)
    
    train_losses_all = [[] for _ in range(num_clients)]
    train_accuracies_all = [[] for _ in range(num_clients)]

    # Initialize global model
    global_model = BrainTumorResNet()
    global_weights = global_model.state_dict()

    weight_shapes = {k: v.shape for k, v in global_weights.items()}

    for round_num in range(num_rounds):
        print(f'Round {round_num + 1}/{num_rounds}')
        local_weights_all = []
        # Train each client model and collect local weights
        for client_idx in range(num_clients):
            local_model = BrainTumorResNet()
            local_model.load_state_dict(global_weights)
            local_weights, train_losses, train_accuracies = train_local_model(local_model, client_loaders[client_idx], epochs=epochs_per_round)
            train_losses_all[client_idx].append(train_losses[-1])
            train_accuracies_all[client_idx].append(train_accuracies[-1])
            local_weights_all.append(local_weights)
            accuracy, _, _ = evaluate_model(local_model, test_loader)
            print(f"Client {client_idx + 1}: Validation Accuracy = {accuracy:.2f}")

        # Encrypt client weights
        encrypted_weights_list = [encrypt_weights(weights, context) for weights in local_weights_all]

        # Aggregate encrypted weights using simple average
        aggregated_encrypted_weights = aggregate_weights_simple(encrypted_weights_list)

        # Decrypt aggregated weights
        decrypted_aggregated_weights = decrypt_weights(aggregated_encrypted_weights, context, weight_shapes)

        # Convert decrypted weights back to torch tensors and update global weights
        global_weights = {k: v for k, v in decrypted_aggregated_weights.items()}
        global_model.load_state_dict(global_weights)

        # Evaluate global model
        global_accuracy, all_preds, all_labels = evaluate_model(global_model, test_loader)
        print(f'Round {round_num + 1}: Global Model Accuracy = {global_accuracy:.2f}')

    # Final evaluation
    global_accuracy, all_preds, all_labels = evaluate_model(global_model, test_loader)
    print(f'Final Global Model Accuracy = {global_accuracy:.2f}')

    # Plot training metrics
    plot_metrics(train_losses_all, train_accuracies_all, num_rounds)

    # Calculate confusion matrix and classification report
    cm, cr = calculate_metrics(all_preds, all_labels)
    print("Confusion Matrix:\n", cm)
    print("Classification Report:\n", cr)

# Fedavg Example usage:
federated_learning_simple_fedavg(num_clients=10, num_rounds=20, epochs_per_round=1)

now = datetime.now()
end_time = now.strftime("%H:%M:%S")

start = datetime.strptime(start_time, "%H:%M:%S") 
end = datetime.strptime(end_time, "%H:%M:%S") 

print("Start Time =", start_time)
print("End Time =", end_time)
difference = end - start 
  
seconds = difference.total_seconds() 
print('difference in seconds is:', seconds) 
  
minutes = seconds / 60
print('difference in minutes is:', minutes) 
  
hours = seconds / (60 * 60) 
print('difference in hours is:', hours)
